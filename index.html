<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yufei Hu - Applying for PhD in Computer Science & Multimodal AI</title>
    <style>
        body {
            text-align: center;
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f9f9f9;
            color: #333;
        }
        .section {
            max-width: 800px;
            margin: 0 auto;
            text-align: left;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            margin-bottom: 30px;
        }
        h1, h2 {
            color: #2c3e50;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        ul {
            list-style-type: none;
            padding: 0;
        }
        li {
            margin-bottom: 10px;
        }
        img {
            width: 150px;
            border-radius: 50%;
            margin-top: 20px;
        }
    </style>
</head>
<body>

    <!-- Home Section -->
    <div class="section">
        <img src="Personnal_Photo.jpg" alt="Yufei Hu">
        <h1>Yufei Hu</h1>
    <p>
        PhD Candidate Applicant in Computer Science & Multimodal AI<br>
        Institut Polytechnique de Paris | ENSTA Paris, France
    </p>

    <p>
        I have achieved my Master’s degree in Engineering from École Nationale Supérieure de Techniques Avancées (ENSTA Paris), and my Bachelor’s degree in Automation from Chang'an University, Xi'an, China.
    </p>

    <p>
        Currently, I am working at CVTE, a publicly listed company, where I have been engaged in algorithm design including machine learning, deep learning, and image processing for approximately two years.
    </p>

    <p>
        My research focuses on advancing <strong>Multimodal AI</strong> systems, with an emphasis on integrating vision-language architectures, optimizing edge computing solutions, and leveraging self-supervised learning techniques. I am particularly passionate about bridging theoretical advancements in AI with practical, real-world applications—especially in industrial automation and resource-constrained edge environments.
    </p>

    <p>
        I am always open to inquiries and collaborations. Please feel free to connect with me.
    </p>

        <p>
            <a href="mailto:yufei.hu.2021@ensta-paris.fr">Email</a> / 
            <a href="https://github.com/flyingU-ai">GitHub</a> / 
            <a href="https://linkedin.com/in/yufei-hu-907598205">LinkedIn</a> / 
            <a href="https://scholar.google.com/citations?user=Ab_GbFwAAAAJ">Google Scholar</a>
        </p>
    </div>

    <!-- Research Interests Section -->
    <div class="section">
        <h2>Research Interests</h2>
        <p>My research interests lie at the intersection of <strong>Multimodal AI</strong>, <strong>Computer Vision</strong>, and <strong>Edge Computing</strong>, with a focus on:</p>
        <ul>
            <li><strong>Multimodal Fusion:</strong> Integrating vision, language, and spatial inputs for robust AI systems.</li>
            <li><strong>Self-Supervised Learning:</strong> Developing methods for learning representations with minimal supervision.</li>
            <li><strong>Model Distillation:</strong> Designing lightweight models for edge deployment.</li>
            <li><strong>Interactive AI:</strong> Building prompt-driven interfaces for human-AI collaboration.</li>
            <li><strong>Uncertainty Quantification:</strong> Enhancing model robustness through uncertainty-aware learning.</li>
        </ul>

<!--         <p>If you have any projects or questions where collaboration could be beneficial, please don't hesitate to reach out to me.</p> -->
    </div>

    <!-- Education Section -->
    <div class="section">
        <h2>Education</h2>
        <ul>
            <li>
                <strong>Ecole Nationale Supérieure de Techniques Avancées (ENSTA Paris)(QS=38)</strong>
                <p>Master of Engineering in Information and Communication Sciences & Technologies</p>
                <p><em>Sep 2019 - Dec 2022</em></p>
            </li>
            <li>
                <strong>Chang'an University, Xi'an, China (Member of Project 211</strong>
                <p>Bachelor of Automation (Information and Control)</p>
                <p><em>Sep 2016 - Sep 2019</em></p>
            </li>
        </ul>
    </div>

<!--     <!-- Research Experience Section -->
    <div class="section">
        <h2>Research Experience</h2>
        <ul>
            <li>
                <strong>Interactive Prompt-Based Object Detection with Cross-Modal Fusion</strong>
                <p>Developed a cross-modal framework integrating user prompts (e.g., clicks) with Vision Transformers via cross-attention and contrastive learning, enhancing adaptability for industrial inspection tasks.</p>
                <p><em>Jan 2024 - Present</em></p>
            </li>
            <li>
                <strong>CNN Model Distillation Based on SAM for Industrial Anomaly Detection</strong>
                <p>Designed a lightweight CNN by distilling knowledge from Segment Anything Model (SAM), achieving 91.3% F1-Score on a high-resolution PCBA dataset with 50ms inference latency on edge devices.</p>
                <p><em>Jun 2022 - Dec 2023</em></p>
            </li>
            <li>
                <strong>Semantic Image Synthesis with CGANs & Transformers</strong>
                <p>Achieved SOTA on Cityscapes, COCO-Stuff, and ADE20K via frequency-aware loss. Submitted to CVPR 2023.</p>
                <p><em>May 2022 - Oct 2022</em></p>
            </li>
        </ul>
    </div> -->
      <div class="section">
        <h2>Research Experience</h2>
        <ul>
            <li>
                <strong>Interactive Prompt-Based Object Detection with Cross-Modal Fusion</strong>
                <p>
                    Lead Researcher<br>
                    Developed a cross-modal framework integrating user prompts with Vision Transformers via cross-attention and contrastive learning. Leveraged shared features for target classification and rotation prediction, ensuring real-time performance. Enhanced adaptability for industrial inspection tasks.
                </p>
                <p><em>Jan 2024 - Present</em></p>
            </li>
            <li>
                <strong>Adaptive CNN Distillation from SAM for Industrial Anomaly Detection</strong>
                <p>
                    Lead Researcher<br>
                    Designed a lightweight CNN by distilling knowledge from the Segment Anything Model (SAM) for high-resolution industrial anomaly detection. Innovatively introduced a threshold-adaptive network structure to eliminate manual parameter tuning, enhancing system stability and robustness.
                </p>
                <p><em>Jun 2022 - Dec 2023</em></p>
            </li>
            <li>
                <strong>Semantic Image Synthesis with CGANs & Transformers</strong>
                <p>
                    Research Intern<br>
                    Achieved SOTA on Cityscapes, COCO-Stuff, and ADE20K via frequency-aware loss. Submitted to CVPR 2023.
                </p>
                <p><em>May 2022 - Oct 2022</em></p>
            </li>
            <li>
                <strong>Neural Architecture Search (NAS) for Deep Morphological Networks</strong>
                <p>
                    Research Intern<br>
                    Developed pseudo-morphological layers using AutoML to enhance CNN performance in edge detection and segmentation (BSD500 dataset). Published in Pattern Recognition (SCI Q1, IF=8.518).
                </p>
                <p><em>May 2020 - Jul 2022</em></p>
            </li>
            <li>
                <strong>Self-Supervised Learning for Semantic Segmentation</strong>
                <p>
                    Research Assistant<br>
                    Developed a self-supervised pipeline using contrastive learning on COCO, achieving state-of-the-art (SOTA) segmentation performance on Cityscapes and BDD100k with minimal supervision. Submitted to Pattern Recognition (SCI Q1).
                </p>
                <p><em>Apr 2021 - Jan 2022</em></p>
            </li>
            <li>
                <strong>Robust Semantic Segmentation via Superpixel-Mix</strong>
                <p>
                    Research Intern<br>
                    Proposed Superpixel-mix, a data augmentation method reducing model uncertainty by 18% under distribution shifts (e.g., adverse weather). Accepted at BMVC 2021 (CCF-B).
                </p>
                <p><em>Jan 2021 - Aug 2021</em></p>
            </li>
        </ul>
    </div>

    <!-- Publications Section -->
    <div class="section">
        <h2>Publications</h2>
        <ul>
            <li>
                <strong>Learning Deep Morphological Networks with Neural Architecture Search</strong>
                <p>Yufei Hu, N. Belkhir, J. Angulo, A. Yao, G. Franchi. <em>Pattern Recognition</em>, 131, 108893, 2022 (SCI Q1, IF=8.518).</p>
                <p><a href="https://arxiv.org/abs/2106.07714">Paper Link</a><a href="https://github.com/ENSTA-U2IS-AI/NAO_morpho">Code Link</a></p>
            </li>
            <li>
                <strong>Robust Semantic Segmentation with Superpixel-Mix</strong>
                <p>G. Franchi, N. Belkhir, M. L. Ha, Yufei Hu, et al. <em>BMVC 2021</em>, 2021 (CCF-B).</p>
                <p><a href="https://arxiv.org/abs/2108.00968">Paper Link</a></p>
            </li>
        </ul>
    </div>

<!--     <!-- Patents Section -->
    <div class="section">
        <h2>Patents</h2>
        <ul>
            <li>
                <strong>Real-time Industrial Foreign Object Detection via Vision Foundation Models</strong>
                <p>Application Number: CN202410901829.2</p>
                <p>Developed a vision foundation model-based framework for real-time detection of foreign objects in industrial environments, achieving high precision in edge deployment.</p>
            </li>
            <li>
                <strong>Height Data Fusion with Texture-Guided Information Recovery and Real-Time Super-Resolution</strong>
                <p>Application Number: CN202410443098.1</p>
                <p>Innovated a multi-sensor fusion method to reconstruct high-resolution height maps using texture guidance, enhancing accuracy in 3D industrial inspection.</p>
            </li>
        </ul>
    </div> -->
    <!-- Patents Section -->
<div class="section">
    <h2>Patents</h2>
    <ul>
        <li>
            <strong>Real-time Industrial Foreign Object Detection via Vision Foundation Models</strong>
            <p>Application Number: CN202410901829.2</p>
            <p>Developed a vision foundation model-based framework for real-time detection of foreign objects in industrial environments, achieving high precision in edge deployment.</p>
        </li>
        <li>
            <strong>Height Data Fusion with Texture-Guided Information Recovery and Real-Time Super-Resolution</strong>
            <p>Application Number: CN202410443098.1</p>
            <p>Innovated a multi-sensor fusion method to reconstruct high-resolution height maps using texture guidance, enhancing accuracy and efficiency in 3D industrial inspection.</p>
        </li>
        <li>
            <strong>Texture-Guided Multi-Sensor Kalman Fusion for 3D Object Reconstruction</strong>
            <p>Application Number: CN202410442633.1</p>
            <p>Designed a fusion framework integrating RGB, depth sensors, and gradient information, guided by RGB to enable high-precision 3D height data fusion using a Kalman filter strategy, providing robust support for 3D industrial inspection.</p>
        </li>
        <li>
            <strong>Height Data Restoration via Gradient-Integrated Fusion</strong>
            <p>Application Number: CN202410443406.0</p>
            <p>Proposed a gradient-aware restoration algorithm for height data recovery in low-resolution industrial scans.</p>
        </li>
    </ul>
</div>


</body>
</html>
